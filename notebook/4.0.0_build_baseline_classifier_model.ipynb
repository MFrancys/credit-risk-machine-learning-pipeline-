{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be368d2c-6221-4d94-92d2-2fc30acb232e",
   "metadata": {},
   "source": [
    "# 3.0.0. Building a Baseline Classifier Model\n",
    "\n",
    "### Methodology\n",
    "\n",
    "In this section, we trained three different baseline classifier models to handle a imbalance dataset, with presence of null values, and nonlinear relationships among features. The models selected for this initial analysis were Logistic Regression, LightGBM (LGBM), and XGBoost (XGBM), because:\n",
    "1. Logistic Regression\n",
    "   Logistic regression is a linear model that estimates probabilities using a logistic function. \n",
    "    - Pros:\n",
    "        - Simplicity and Interpretability: Easy to implement and results are interpretable.\n",
    "        - Efficient Training: Less computationally intensive compared to tree-based models.\n",
    "    - Cons:\n",
    "        - Handling of Non-Linear Features: Performs poorly if relationships between features and the target are non-linear unless extensive feature engineering is done.\n",
    "        - Requirement for Feature Scaling: Logistic regression requires feature scaling to perform well, as it is sensitive to the magnitude of input features.\n",
    "        - Cannot Handle Missing Values Directly: Requires complete data or imputation of missing values before training.\n",
    "        \n",
    "2. LightGBM (LGBM)\n",
    "    LightGBM is a gradient boosting framework that uses tree-based learning algorithms.\n",
    "    - Pros:\n",
    "        - Scalability: Works well with large datasets and supports GPU learning.\n",
    "        - Performance: Generally provides high performance, especially on datasets where the relationship between variables is complex and non-linear.\n",
    "        - Efficiency with Large Datasets: Optimized to run faster and use less memory compared to other gradient boosting frameworks.\n",
    "        - Handling of Missing Values: Natively handles missing values without requiring imputation.\n",
    "        - Robust to Feature Scaling: Automatically handles varying scales of data, making it less sensitive to the need for feature normalization.\n",
    "    - Cons:\n",
    "        - Overfitting: Prone to overfitting, especially with small data sets.\n",
    "        - Parameter Tuning: Requires careful tuning of parameters and sometimes extensive hyperparameter optimization.\n",
    "\n",
    "3. XGBoost (XGBM)\n",
    "   XGBoost also uses gradient boosting algorithms but is known for its ability to do parallel processing, tree pruning, handling missing values, and regularizing to avoid overfitting.\n",
    "    - Pros\n",
    "        - Handling Irregularities: Good at handling missing values and various data irregularities.\n",
    "        - Model Strength: Regularization helps to prevent overfitting and provides robust performance across various types of data.\n",
    "    \n",
    "    - Cons\n",
    "        - Computational Intensity: Can be resource-intensive in terms of computation, especially with large data sets and a deep number of trees.\n",
    "        - Complexity: More parameters to tune compared to other models, which can make it harder to find the right model configuration.\n",
    "\n",
    "\n",
    "These are the parameters tailored for each models:\n",
    "1. Logistic Regression Parameters:\n",
    "  Penalty: 'l2' (L2 regularization to prevent overfitting)\n",
    "  C: 1.0 (Regularization strength; smaller values specify stronger regularization)\n",
    "2. LightGBM Parameters:\n",
    "    Num_leaves: 34 (Number of leaves in one tree; controls model complexity)\n",
    "3. XGBoost Parameters:\n",
    "    Max_depth: 3 (Maximum depth of a tree; ensures the model is sufficiently deep to learn relationships)\n",
    "    Subsample: 0.8 (Subsample ratio of the training instances; prevents overfitting)\n",
    "    Colsample_bytree: 0.8 (Subsample ratio of columns when constructing each tree; provides feature sampling)\n",
    "\n",
    "We evaluated the models using ROC AUC Score, PR AUC Score, and the Kolmogorov-Smirnov (KS) statistic to gauge their ability to distinguish between classes and predict the minority class in an unbalanced dataset.\n",
    "\n",
    "### Conclusion\n",
    "The following results were observed across the models, showing the pros and cons of each model:\n",
    "\n",
    "| Model                | ROC AUC Score | PR AUC Score | KS Statistic | Handles Nulls | Sensitive to Scale | Overfitting Risk | Computational Intensity |\n",
    "|----------------------|---------------|--------------|--------------|---------------|--------------------|------------------|-------------------------|\n",
    "| Logistic Regression  | 0.50000       | 0.19943      | 0.00000      | No            | Yes                | Low              | Low                     |\n",
    "| LightGBM (LGBM)      | 0.564588      | 0.255012     | 0.116133     | Yes           | No                 | Medium           | Medium                  |\n",
    "| XGBoost (XGBM)       | 0.618952      | 0.280595     | 0.212456     | Yes           | No                 | High             | High                    |\n",
    "\n",
    "\n",
    "Based on the evaluation metrics, XGBM outperforms the other models with the highest ROC AUC, PR AUC, and KS scores, making it the most suitable choice given the complexity of the dataset with unbalanced data and non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2375c2e-eae9-4171-90eb-b4b82433449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78434480-6ad8-4e7d-a0a1-1026e2b6f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test: np.ndarray, preds: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates several key performance metrics for evaluating a classification model.\n",
    "\n",
    "    This function computes the following metrics:\n",
    "    - ROC AUC Score: The area under the Receiver Operating Characteristic curve, useful for assessing the \n",
    "      overall effectiveness of the predictions with respect to the true outcomes.\n",
    "    - PR AUC Score: The area under the Precision-Recall curve, useful for datasets with a significant imbalance.\n",
    "    - Kolmogorov-Smirnov Statistic (KS): Measures the degree to which the distributions of the predicted \n",
    "      probabilities of the positive and negative classes differ.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test (np.ndarray): An array containing the actual binary labels of the test data (0 or 1).\n",
    "    - preds (np.ndarray): An array containing the predicted probabilities corresponding to the likelihood \n",
    "      of the positive class (class label 1).\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing the calculated metrics:\n",
    "        * 'roc_auc_score': float, representing the ROC AUC score.\n",
    "        * 'pr_auc': float, representing the precision-recall AUC score.\n",
    "        * 'ks': float, representing the Kolmogorov-Smirnov statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'roc_auc_score': metrics.roc_auc_score(y_true=y_test, y_score=preds),\n",
    "        'pr_auc': metrics.average_precision_score(y_true=y_test, y_score=preds),\n",
    "        'ks': ks_2samp(preds[y_test == 0], preds[y_test == 1])[0],\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n",
    "    \n",
    "     \n",
    "def get_logistic_regression_pipeline(numeric_features, **model_parameters):\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=0), \n",
    "        MinMaxScaler()\n",
    "    )\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", LogisticRegression(**model_parameters, random_state=split_seed))\n",
    "        ]\n",
    "    )\n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0f658-b45a-4b24-8905-7fc96f1c8764",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1901301d-588e-4701-b467-106722d9974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': {'penalty': 'l2', 'C': 1.0},\n",
       " 'lgbm': {'objective': 'binary',\n",
       "  'boosting_type': 'gbdt',\n",
       "  'metric': 'auc',\n",
       "  'num_leaves': 31,\n",
       "  'learning_rate': 0.05,\n",
       "  'feature_fraction': 0.8,\n",
       "  'bagging_fraction': 0.8,\n",
       "  'bagging_freq': 5,\n",
       "  'is_unbalance': True},\n",
       " 'xgbm': {'objective\"': 'binary:logistic',\n",
       "  'booster\"': 'gbtree',\n",
       "  'eval_metric\"': 'auc',\n",
       "  'eta': 0.01,\n",
       "  'gamma': 0.1,\n",
       "  'max_depth': 6,\n",
       "  'min_child_weight': 3,\n",
       "  'subsample': 0.8,\n",
       "  'colsample_bytree': 0.8,\n",
       "  'scale_pos_weight': 4,\n",
       "  'lambda': 1,\n",
       "  'alpha': 0.1,\n",
       "  'max_delta_step': 1,\n",
       "  'n_estimators': 100}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "numeric_features = config[\"features\"][\"numerical\"]\n",
    "features = numeric_features\n",
    "target = config[\"main\"][\"target\"]\n",
    "data_train_path = Path.cwd().parent / config[\"main\"][\"data_train_path\"]\n",
    "train_validation_path = Path.cwd().parent / config[\"main\"][\"data_validation_path\"]\n",
    "\n",
    "train_df = pd.read_pickle(data_train_path)\n",
    "validation_df = pd.read_pickle(train_validation_path)\n",
    "split_seed = config[\"main\"][\"random_seed\"]\n",
    "model_parameters = config[\"model_parameters\"]\n",
    "\n",
    "model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf8cce76-cf23-4aa6-b0cc-6e76df4e0945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9479, 142)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, Y_train = train_df[features], train_df[target]\n",
    "X_val, Y_val = validation_df[features], validation_df[target]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b000a3f-769f-4aad-8f71-c75c34b64ec2",
   "metadata": {},
   "source": [
    "### 2. Train logit regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb53c5c-8107-4be6-a069-7f0d11ce109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498e4a86-5889-4a4c-8304-3d6e6a980ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logist': {'roc_auc_score': 0.49940688018979834,\n",
       "  'pr_auc': 0.19943019943019943,\n",
       "  'ks': 0.0011862396204033216}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = config[\"model_parameters\"]['logistic_regression']\n",
    "\n",
    "pipe = get_logistic_regression_pipeline(numeric_features, **params) \n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "logist_preds = pipe.predict(X_val[features])\n",
    "\n",
    "model_results[\"logist\"] = calculate_metrics(Y_val, logist_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78744f6a-6e3b-430a-b942-2a99c320e38c",
   "metadata": {},
   "source": [
    "### 3. Train lightgbm regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a412fa8-b7a2-4da2-8ec5-0027347b63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Info] Number of positive: 1839, number of negative: 7640\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004577 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 22286\n",
      "[LightGBM] [Info] Number of data points in the train set: 9479, number of used features: 124\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.194008 -> initscore=-1.424176\n",
      "[LightGBM] [Info] Start training from score -1.424176\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logist': {'roc_auc_score': 0.49940688018979834,\n",
       "  'pr_auc': 0.19943019943019943,\n",
       "  'ks': 0.0011862396204033216},\n",
       " 'lgbm': {'roc_auc_score': 0.5737643337287466,\n",
       "  'pr_auc': 0.24679150727049412,\n",
       "  'ks': 0.15521098118962887}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = config[\"model_parameters\"][\"lgbm\"]\n",
    "lgbm_model = LGBMClassifier(**params, random_state=split_seed)\n",
    "lgbm_model.fit(X_train, Y_train)\n",
    "lgbm_preds = lgbm_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "model_results[\"lgbm\"] = calculate_metrics(Y_val, lgbm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31feb4f-b57d-4a2d-9b66-6b5ac1feff79",
   "metadata": {},
   "source": [
    "### 4. Train xgboost regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c95793-e570-4393-ad06-8ad802b3c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:05:34] WARNING: /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/learner.cc:627: \n",
      "Parameters: { \"booster\"\", \"eval_metric\"\", \"objective\"\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = config[\"model_parameters\"][\"xgbm\"]\n",
    "xgbm_model = XGBClassifier(missing=np.nan, **params, random_state=split_seed)\n",
    "\n",
    "xgbm_model.fit(X_train, Y_train)\n",
    "xgbm_preds = xgbm_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "model_results[\"xgbm\"] = calculate_metrics(Y_val, xgbm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bdb180-d39a-451f-8a4f-103138e1b9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logist</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>xgbm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc_score</th>\n",
       "      <td>0.499407</td>\n",
       "      <td>0.573764</td>\n",
       "      <td>0.599619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr_auc</th>\n",
       "      <td>0.199430</td>\n",
       "      <td>0.246792</td>\n",
       "      <td>0.269709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ks</th>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.155211</td>\n",
       "      <td>0.204050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 logist      lgbm      xgbm\n",
       "roc_auc_score  0.499407  0.573764  0.599619\n",
       "pr_auc         0.199430  0.246792  0.269709\n",
       "ks             0.001186  0.155211  0.204050"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5bd61-c4b6-464d-aeaa-1a81e4afa663",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-credit-risk-machine-learning-pipeline",
   "language": "python",
   "name": "myenv-credit-risk-machine-learning-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
