{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57668e1d-47b6-47a5-accf-1ebdcaea4a39",
   "metadata": {},
   "source": [
    "# 2.0.0 - Data Split\n",
    "\n",
    "### Methodology\n",
    "This section details the process of splitting the dataset into training, validation, and testing sets. The dataset was split using a time-based strategy to segregate the data into distinct periods for training, validation, and testing. Furthermore, a spatial segregation approach was employed within the training data to create a validation set.\n",
    "\n",
    "### Conclusion:\n",
    "- Data Split\n",
    "    - Training Set: Data from July 2022 to February 2023, used for initial model training.\n",
    "    - Validation Set: A subset of the training set, spatially distinct, used for tuning model parameters and initial evaluation.\n",
    "    - Testing Set: Data from March 2023 to April 2024, used to simulate model performance on future, unseen data.\n",
    "\n",
    "\n",
    "- Data Distribution\n",
    "The split resulted in the following distribution of samples:\n",
    "\n",
    "    - Training Set: 9479 samples with a default rate (bads) of 19.4%.\n",
    "    - Validation Set: 1053 samples with a default rate of 19.94%.\n",
    "    - Testing Set: 3845 samples with a default rate of 16.9%.\n",
    "\n",
    "This distribution ensures that each set is representative of the overall data, with the training set encompassing the majority of the data (65.59%), followed by the testing set (26.7%) and the validation set (7.3%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022c534-b663-460b-9bcd-bf35bd5f9515",
   "metadata": {},
   "source": [
    "### 1. Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b810d590-b824-4a5f-9f71-81c41723837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1038320-a794-4435-a8c3-0722086c2a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_split_dataset(\n",
    "    dataset: pd.DataFrame,\n",
    "    train_start_date: str,\n",
    "    train_end_date: str,\n",
    "    holdout_end_date: str,\n",
    "    time_column: str,\n",
    "    space_column: str,\n",
    "    holdout_start_date: str = None,\n",
    "    split_seed: int = 42,\n",
    "    space_holdout_percentage: float = 0.1,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Splits a dataset into training, validation, and testing sets based on time and space dimensions.\n",
    "\n",
    "    The function first segregates the data into time-based training and testing intervals. \n",
    "    Within the training interval, it further splits the data into training and validation sets \n",
    "    based on a specified percentage of unique values in the 'space_column', ensuring that \n",
    "    the validation set is spatially distinct from the training set.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (pd.DataFrame): The complete dataset to split.\n",
    "    - train_start_date (str): The start date for the training dataset.\n",
    "    - train_end_date (str): The end date for the training dataset.\n",
    "    - holdout_end_date (str): The end date for the testing dataset.\n",
    "    - time_column (str): The column in the dataset that contains the time information.\n",
    "    - space_column (str): The column in the dataset that represents the spatial information.\n",
    "    - holdout_start_date (str, optional): The start date for the testing dataset. Defaults to train_end_date.\n",
    "    - split_seed (int, optional): The seed for the random state used in spatial sampling. Defaults to 42.\n",
    "    - space_holdout_percentage (float, optional): The percentage of the space_column's unique values \n",
    "      to hold out for validation. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing three pd.DataFrame objects:\n",
    "        1. train_set: The training dataset.\n",
    "        2. validation_set: The spatially distinct validation dataset.\n",
    "        3. test_set: The testing dataset set aside by time.\n",
    "    \"\"\"\n",
    "\n",
    "    state = rnd.RandomState(split_seed)\n",
    "    holdout_start_date = holdout_start_date if holdout_start_date else train_end_date\n",
    "    train_set = dataset[\n",
    "        (dataset[time_column] >= train_start_date)\n",
    "        & (dataset[time_column] < train_end_date)\n",
    "    ]\n",
    "    train_period_space = train_set[space_column].unique()\n",
    "    test_set = dataset[\n",
    "        (dataset[time_column] >= holdout_start_date)\n",
    "        & (dataset[time_column] < holdout_end_date)\n",
    "    ]\n",
    "    validation_idx = state.choice(\n",
    "        a=train_period_space,\n",
    "        size=int(space_holdout_percentage * len(train_period_space)),\n",
    "        replace=False,\n",
    "    )\n",
    "    validation_set = train_set[train_set[space_column].isin(validation_idx)]\n",
    "    train_set = train_set[~train_set[space_column].isin(validation_idx)]\n",
    "\n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f046c24-ff34-4987-a2d9-2389c14c44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path.cwd().parent / \"data\"\n",
    "MAIN_DATASET_PATH = DATA_PATH / \"processed/202404_final_dataset.pickle\"\n",
    "\n",
    "\n",
    "train_start_date = \"2022-07\"\n",
    "train_end_date = \"2023-03\"\n",
    "holdout_end_date = \"2023-05\"\n",
    "time_column = \"loan_origination_datetime_month\"\n",
    "space_column = \"customer_id\"\n",
    "target = \"target\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d113d8b2-1828-4c5a-bca2-ad5ff9660db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14454 entries, 0 to 14453\n",
      "Columns: 291 entries, customer_id to credit_reports__debt_due_ratio\n",
      "dtypes: float64(286), int64(4), object(1)\n",
      "memory usage: 32.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(MAIN_DATASET_PATH)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844e7320-7096-4b45-9f52-a716d6432b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = time_split_dataset(\n",
    "    df,\n",
    "    train_start_date=train_start_date,\n",
    "    train_end_date=train_end_date,\n",
    "    holdout_end_date=holdout_end_date,\n",
    "    time_column=time_column,\n",
    "    space_column=space_column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70689ab-2136-42b6-8427-6b01deba0c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_samples</th>\n",
       "      <th>bads</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>target_distribution</th>\n",
       "      <th>samples_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>9479</td>\n",
       "      <td>1839</td>\n",
       "      <td>2022-07</td>\n",
       "      <td>2023-02</td>\n",
       "      <td>0.194008</td>\n",
       "      <td>0.659317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>1053</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-07</td>\n",
       "      <td>2023-02</td>\n",
       "      <td>0.199430</td>\n",
       "      <td>0.073242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>3845</td>\n",
       "      <td>650</td>\n",
       "      <td>2023-03</td>\n",
       "      <td>2023-04</td>\n",
       "      <td>0.169051</td>\n",
       "      <td>0.267441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_samples  bads      min      max  target_distribution  \\\n",
       "train            9479  1839  2022-07  2023-02             0.194008   \n",
       "validation       1053   210  2022-07  2023-02             0.199430   \n",
       "test             3845   650  2023-03  2023-04             0.169051   \n",
       "\n",
       "            samples_dist  \n",
       "train           0.659317  \n",
       "validation      0.073242  \n",
       "test            0.267441  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_len = len(train_df)\n",
    "val_len = len(validation_df)\n",
    "test_len = len(test_df)\n",
    "\n",
    "train_bads = train_df.target.sum()\n",
    "val_bads = validation_df.target.sum()\n",
    "test_bads = test_df.target.sum()\n",
    "\n",
    "train_min = train_df[time_column].min()\n",
    "val_min = validation_df[time_column].min()\n",
    "test_min = test_df[time_column].min()\n",
    "\n",
    "\n",
    "train_max = train_df[time_column].max()\n",
    "val_max = validation_df[time_column].max()\n",
    "test_max = test_df[time_column].max()\n",
    "\n",
    "train_dist = train_df[target].mean()\n",
    "val_dist = validation_df[target].mean()\n",
    "test_dist = test_df[target].mean()\n",
    "\n",
    "(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"n_samples\": [train_len, val_len, test_len],\n",
    "            \"bads\": [train_bads, val_bads, test_bads],\n",
    "            \"min\": [train_min, val_min, test_min],\n",
    "            \"max\": [train_max, val_max, test_max],\n",
    "            \"target_distribution\": [train_dist, val_dist, test_dist],\n",
    "        },\n",
    "        index=[\"train\", \"validation\", \"test\"],\n",
    "    ).assign(samples_dist=lambda x: x[\"n_samples\"] / sum(x[\"n_samples\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19684da-7b41-4705-afbc-7a0d2d00befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_date = str(MAIN_DATASET_PATH).split(\"/\")[-1:][0][0:6]\n",
    "train_df.to_pickle(DATA_PATH / f\"processed/{dataset_date}_train_data.pickle\")\n",
    "validation_df.to_pickle(DATA_PATH / f\"processed/{dataset_date}_validation_data.pickle\")\n",
    "test_df.to_pickle(DATA_PATH / f\"processed/{dataset_date}_test_data.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk-machine-learning-pipeline",
   "language": "python",
   "name": "credit-risk-machine-learning-pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
